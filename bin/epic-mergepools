#!/usr/bin/env python

"""
epic-mergepools

UNRELEASED SOFTWARE DO NOT DISTRIBUTE!
(Visit github.com/endrebak/epic for examples and help.)

Usage:
    epic-mergepools [--min-islands=MIN] [--nb-cpu=CPU] FILE...
    epic-mergepools --help

Arguments:
    FILE                        a list of bin files to merge

Options:
    -h --help                   show this help message
    -m MIN --min-islands=MIN    min nb of samples that need to consider the bin as part of an island for the bin to be
                                included in the final matrix [default: 1]
                                (can also be a comma-separated list of numbers start,stop[,step] like 1,5 or 2,6,2
                                which will output results for each --min-island)
    -n CPU --nb-cpu=CPU         how many cores to use [default: 1]

"""

from __future__ import print_function
from collections import defaultdict

from natsort import natsorted
from sys import stdout
from epic.config import logging_settings
from joblib import Parallel, delayed


import pandas as pd
# import matplotlib.pyplot as plt
from os.path import expanduser

from docopt import docopt

import logging


from epic.config.cache_settings import MEMORY

# TODO: are the custom merge methods actually faster than Pandas builtin?

def read_files_as_dfs_and_put_into_list(bin_files):

    dfs = []
    for bin_file in bin_files:
        df = pd.read_table(bin_file, index_col=[0, 1], engine="c")
        logging.info(df.head().to_csv(sep=" "))
        dfs.append(df)

    return dfs


def horizontally_merge_island_count_per_chromosome(chromosome, islands_dfs):

    logging.info("Merging " + chromosome)
    islands_in_each_file_iter = iter(islands_dfs)
    island_merge_df = next(islands_in_each_file_iter)
    for islands in islands_in_each_file_iter:
        island_merge_df = island_merge_df.merge(islands, how="outer", left_index=True, right_index=True).fillna(0)

    return island_merge_df


def horizontally_merge_island_count_data(island_count_pr_bin_per_chromosome, nb_cpu):

    merged_dfs_per_chromosome = Parallel(n_jobs=nb_cpu)(delayed(horizontally_merge_island_count_per_chromosome)(chromosome, island_dfs) for chromosome, island_dfs in island_count_pr_bin_per_chromosome.items())

    return pd.concat(merged_dfs_per_chromosome).fillna(0).sum(axis=1)


def get_number_of_islands_per_bin(dfs):

    island_count_pr_bin = defaultdict(list)

    for df in dfs:
        for chromosome in natsorted(df.index.get_level_values("Chromosome").drop_duplicates()):
            island_count_pr_bin[chromosome].append(pd.DataFrame(df.Island))

    return island_count_pr_bin


def get_island_data_from_dfs(dfs):

    islands_data = defaultdict(list)

    for df in dfs:
        islands = df.drop("Island", axis=1)
        for chromosome in natsorted(df.index.get_level_values("Chromosome").drop_duplicates()):
            islands_data[chromosome].append(islands.xs(chromosome, level="Chromosome", drop_level=False))

    return islands_data


def horizontally_merge_island_data_per_chromosome(chromosome, islands_dfs):

    logging.info("Merging " + chromosome)
    islands_in_each_file_iter = iter(islands_dfs)
    island_merge_df = next(islands_in_each_file_iter)
    for islands in islands_in_each_file_iter:
        island_merge_df = island_merge_df.merge(islands, how="outer", left_index=True, right_index=True).fillna(0)

    return island_merge_df


def horizontally_merge_island_data(islands_data, nb_cpu):

    merged_dfs_per_chromosome = Parallel(n_jobs=nb_cpu)(delayed(horizontally_merge_island_data_per_chromosome)(chromosome, island_dfs) for chromosome, island_dfs in islands_data.items())

    return pd.concat(merged_dfs_per_chromosome)


if __name__ == '__main__':

    # TODO:
    # This algorithm is not linear. Split into chromosomes to get better
    # speed (albeit not complexity). Also can then run each chromosome in parallel.

    args = docopt(__doc__)

    bin_files = args["FILE"]
    nb_cpu = int(args["--nb-cpu"])

    min_islands = int(args["--min-islands"]) #.split(",")

    logging.info("Reading input files.")
    dfs = read_files_as_dfs_and_put_into_list(bin_files)

    logging.info("Extracting the bin count data from the files.")
    islands_data = get_island_data_from_dfs(dfs)

    logging.info("Merging the bin count data horizontally.")
    main_islands = horizontally_merge_island_data(islands_data, nb_cpu)

    logging.info("Collecting number of islands in each bin.")
    island_count_pr_bin_per_chromosome = get_number_of_islands_per_bin(dfs)

    logging.info("Merging the island count data horizontally.")
    island_count_pr_bin = horizontally_merge_island_count_data(island_count_pr_bin_per_chromosome, nb_cpu)

    logging.info("Merge data to produce final df.")
    main_islands.insert(0, "Island", island_count_pr_bin)

    main_islands.astype(int).to_csv(stdout, sep="\t")

    # if False:
    #     nonzeroes_in_each_bin = (main_islands != 0).sum(axis=1)

    #     counts = nonzeroes_in_each_bin.value_counts()
    #     counts.sort_values()

    #     p = sns.countplot(nonzeroes_in_each_bin, order=sorted(counts.index))
    #     fig = p.get_figure()

    #     fig.savefig(expanduser("islands.pdf"))

    #     too_few_islands = main_islands.loc[main_islands.Island < min_islands].index

    #     # Normalize before dropping
    #     norm_main_islands = main_islands.drop("Island", axis=1)
    #     colsums_norm_main_islands = norm_main_islands.sum()
    #     norm_main_islands = (norm_main_islands/colsums_norm_main_islands) * 1e6

    #     norm_main_islands = norm_main_islands.drop(too_few_islands)
    #     norm_main_islands = norm_main_islands.sub(norm_main_islands.median(axis=1), axis=0)
