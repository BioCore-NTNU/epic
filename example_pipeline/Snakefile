from snakemake.shell import shell

shell.executable("bash")

import pandas as pd

samples = "1 2 3".split()
cells = "fibroblast keratinocyte melanocyte".split()
colors = "77,175,74 228,26,28 55,126,184".split()

color_dict = {k: v for k, v in zip(cells, colors)}

prefix = config["prefix"]

sample_sheet = pd.read_table(config["sample_sheet"], sep=" ")
ss = sample_sheet

wildcard_constraints:
    cell = "({})".format("|".join(ss.Group.drop_duplicates()))


rule all:
    input:
        f"{prefix}/data/epic-cluster/epic/matrixes.gz",
        expand("{prefix}/data/epic-merge/{caller}/matrixes.gz",
               prefix = prefix,
               cell=cells,
               caller="epic".split()), # macs2
        # f"{prefix}/data/epic-merge/test/matrixes.gz",
        f"{prefix}/data/epic/chip_bw/all.bw",
        f"{prefix}/data/epic/input_bw/all.bw",
        f"{prefix}/data/epic-cluster/epic/clusters.bigwig",
        expand("{prefix}/data/publish/{cell}.bb",
               prefix = prefix,
               cell=cells,
               caller="epic".split()), # macs2


def get_files(w, chip):

    celltype = w.cell

    files = list(ss.loc[(ss.ChIP == chip) & (ss.Group == celltype)].Path)

    return files


def get_all_files(chip):

    files = list(ss.loc[ss.ChIP == chip].Path)

    return files


rule run_epic:
    input:
        chip = lambda w: get_files(w, "ChIP"),
        input = lambda w: get_files(w, "Input"),
    output:
        csv = "{prefix}/data/epic/{cell}.csv",
        bed = "{prefix}/data/epic/{cell}.bed",
        matrix = "{prefix}/data/epic/{cell}.matrix.gz",
        log2_fc_bigwig = "{prefix}/data/epic/log2_fc_{cell}.bw",
        input_bigwig = "{prefix}/data/epic/input_bw/{cell}.bw",
        chip_bigwig = "{prefix}/data/epic/chip_bw/{cell}.bw"
    params:
        individual_bigwig = "{prefix}/data/epic/bw/",
        individual_log2_fc_bigwig = "{prefix}/data/epic/i2bw/"
    threads:
        48
    shell:
        "epic -cpu {threads} -g 3 -w "
        "200 -gn hg19 " # -fs 200 "
        "-fs 200 "
        "-b {output.bed} -t {input.chip} -c {input.input} "
        "-sm {output.matrix} "
        " --bigwig {params.individual_bigwig} "
        " -i2bw {params.individual_log2_fc_bigwig} "
        " --log2fc-bigwig {output.log2_fc_bigwig} "
        " --input-bigwig {output.input_bigwig} "
        " --chip-bigwig {output.chip_bigwig} "
        "> {output.csv}"


rule create_chip_input_sum:
    input:
        chip = get_all_files("ChIP"),
        input = get_all_files("Input")
    output:
        sum_chip = "{prefix}/data/epic/chip_bw/all.bw",
        sum_input = "{prefix}/data/epic/input_bw/all.bw"
    threads:
        48
    shell:
        "epic -cpu {threads} -g 3 -w "
        "200 -gn hg19 "
        "-fs 200 "
        "-t {input.chip} -c {input.input} "
        " --input-bigwig {output.sum_input} "
        " --chip-bigwig {output.sum_chip} "
        "> /dev/null"


rule run_macs2:
    input:
        chip = lambda w: get_files(w, "ChIP"),
        input = lambda w: get_files(w, "Input"),
    output:
        bed = "{prefix}/data/macs2/{cell}.bed",
    params:
        prefix = "{prefix}/data/macs2/{cell}",
        outfile = "{prefix}/data/macs2/{cell}_peaks.broadPeak"
    conda:
        "envs/macs2.yaml"
    shell:
        "macs2 callpeak --nomodel --extsize 200 --broad -t {input.chip} -c {input.input} -n {params.prefix} && mv {params.outfile} {output[0]}"


rule run_epic_merge:
    input:
        matrixes = expand("{{prefix}}/data/epic/{cell}.matrix.gz",
                          cell=cells),
        bed = expand("{{prefix}}/data/{{caller}}/{cell}.bed",
                     cell=cells)
    output:
        "{prefix}/data/epic-merge/{caller}/matrixes.gz"
    threads:
        24
    shell:
        "epic-merge -cpu {threads} -r {input.bed} -o {output[0]} -m {input.matrixes}"


rule test_epic_merge:
    input:
        matrixes = expand("{{prefix}}/data/epic/{cell}.matrix.gz",
                            cell=cells),
    output:
        "{prefix}/data/epic-merge/test/matrixes.gz"
    threads:
        24
    shell:
        "epic-merge -cpu {threads} -o {output[0]} -m {input.matrixes}"


rule epic_cluster:
    input:
        "{prefix}/data/epic-merge/{caller}/matrixes.gz"
    output:
        matrix = "{prefix}/data/epic-cluster/{caller}/matrixes.gz",
        bed = "{prefix}/data/epic-cluster/{caller}/clusters.bed",
        bigwig = "{prefix}/data/epic-cluster/{caller}/clusters.bigwig"
    shell:
        "epic-cluster -d 600 -m {input[0]} -o {output.matrix} -B {output.bed} -t 1 -cpu 25 -bw {output.bigwig} -g hg19"


rule add_color_to_bedfiles:
    input:
        "{prefix}/data/epic/{cell}.bed"
    output:
        "{prefix}/data/colored_bed/{cell}.bed"
    params:
        rgb = lambda w: color_dict[w.cell]
    shell:
        "awk '{{print $1, $2, $3, $4, 0, $6, $2, $3, \"{params.rgb}\"}}' {input[0]} | tr ' ' '\t' | sort -k1,1 -k2,2n > {output[0]}"


rule get_chromsizes:
    output:
        "{prefix}/data/chromsizes/hg19.txt"
    shell:
        "fetchChromSizes hg19 > {output}"


rule bed_to_bigbed:
    input:
        bb = "{prefix}/data/colored_bed/{cell}.bed",
        cs = "{prefix}/data/chromsizes/hg19.txt"
    output:
        "{prefix}/data/publish/{cell}.bb"
    shell:
        "bedToBigBed {input.bb} {input.cs} {output[0]}"


rule create_trunks_flanks_valleys_bed_with_color:
    input:
        "{prefix}/data/epic-cluster/{caller}/clusters.bed"
    output:
        "{prefix}/data/epic-cluster/publish/{caller}/clusters.bed"
    script:
        "scripts/add_color_trunks_flanks_valleys_bed.py"


rule turn_trunks_flanks_valleys_bed_into_bigbed:
    input:
        bb = "{prefix}/data/epic-cluster/publish/{caller}/clusters.bed",
        cs = "{prefix}/data/chromsizes/hg19.txt"
    output:
        "{prefix}/data/epic-cluster/publish/{caller}/clusters.bb"
    shell:
        "bedToBigBed {input.bb} {input.cs} {output[0]}"
